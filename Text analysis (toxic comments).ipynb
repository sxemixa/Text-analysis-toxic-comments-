{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Балансировка-данных\" data-toc-modified-id=\"Балансировка-данных-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Балансировка данных</a></span><ul class=\"toc-item\"><li><span><a href=\"#Уменьшение-размера-весов-классов\" data-toc-modified-id=\"Уменьшение-размера-весов-классов-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Уменьшение размера весов классов</a></span></li><li><span><a href=\"#Ресемплинг-с-уменшением-класса-0\" data-toc-modified-id=\"Ресемплинг-с-уменшением-класса-0-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Ресемплинг с уменшением класса 0</a></span></li></ul></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Классификатор-LogisticRegression\" data-toc-modified-id=\"Классификатор-LogisticRegression-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Классификатор LogisticRegression</a></span></li><li><span><a href=\"#Классификатор-DecisionTreeClassifier\" data-toc-modified-id=\"Классификатор-DecisionTreeClassifier-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Классификатор DecisionTreeClassifier</a></span></li><li><span><a href=\"#Классификатор-CatBoostClassifier\" data-toc-modified-id=\"Классификатор-CatBoostClassifier-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Классификатор CatBoostClassifier</a></span></li><li><span><a href=\"#Вывод-по-классификаторам\" data-toc-modified-id=\"Вывод-по-классификаторам-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Вывод по классификаторам</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "toxic_comments = pd.read_csv('C:/Users/mkhasanov/GitHub/datasets/toxic_comments.csv')\n",
    "toxic_comments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143346\n",
       "1     16225\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8.834884437596301"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Посмотрим сколько у нас токсичных/нектоксичных текстов\n",
    "display(toxic_comments['toxic'].value_counts())\n",
    "#Выведем соотношение\n",
    "class_ratio = toxic_comments['toxic'].value_counts()[0] / toxic_comments['toxic'].value_counts()[1]\n",
    "class_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "Видим что классы несбалансированы, применем несколько спомобов балансировки и сравним их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mkhasanov/nltk_data'\n    - 'c:\\\\users\\\\mkhasanov\\\\miniconda3\\\\nltk_data'\n    - 'c:\\\\users\\\\mkhasanov\\\\miniconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\users\\\\mkhasanov\\\\miniconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mkhasanov\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\mkhasanov\\miniconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mkhasanov\\miniconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mkhasanov/nltk_data'\n    - 'c:\\\\users\\\\mkhasanov\\\\miniconda3\\\\nltk_data'\n    - 'c:\\\\users\\\\mkhasanov\\\\miniconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\users\\\\mkhasanov\\\\miniconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mkhasanov\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ab499ba15bc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleared_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtoxic_comments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lemm_text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoxic_comments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatize_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtoxic_comments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoxic_comments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mkhasanov\\miniconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4428\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4429\u001b[0m         \"\"\"\n\u001b[1;32m-> 4430\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4432\u001b[0m     def _reduce(\n",
      "\u001b[1;32mc:\\users\\mkhasanov\\miniconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1080\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mkhasanov\\miniconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1135\u001b[0m                 \u001b[1;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m                 \u001b[1;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1137\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1138\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mkhasanov\\miniconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-ab499ba15bc0>\u001b[0m in \u001b[0;36mlemmatize_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mlemm_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mcleared_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[^a-zA-Z]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemm_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleared_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mkhasanov\\miniconda3\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mword\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \"\"\"\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mkhasanov\\miniconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mkhasanov\\miniconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mkhasanov\\miniconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mkhasanov\\miniconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mkhasanov/nltk_data'\n    - 'c:\\\\users\\\\mkhasanov\\\\miniconda3\\\\nltk_data'\n    - 'c:\\\\users\\\\mkhasanov\\\\miniconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\users\\\\mkhasanov\\\\miniconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mkhasanov\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = text.lower()\n",
    "    lemm_text = \"\".join(lemmatizer.lemmatize(text))\n",
    "    cleared_text = re.sub(r'[^a-zA-Z]', ' ', lemm_text) \n",
    "    return \" \".join(cleared_text.split())\n",
    "\n",
    "toxic_comments['lemm_text'] = toxic_comments['text'].apply(lemmatize_text)\n",
    "\n",
    "toxic_comments = toxic_comments.drop(['text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разобьем выборку по отношению 50/20/20. Уменьшим количество кроссвалидаций до 3 из-за размера выборки.\n",
    "target = toxic_comments['toxic']\n",
    "features = toxic_comments.drop(['toxic'], axis=1)\n",
    "\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(features, \n",
    "                                                                              target, \n",
    "                                                                              test_size=0.5, \n",
    "                                                                              random_state=1515)\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(features_valid, \n",
    "                                                                            target_valid, \n",
    "                                                                            test_size=0.5,\n",
    "                                                                            random_state=1515)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "\n",
    "features_train = count_tf_idf.fit_transform(features_train['lemm_text'].values.astype('U'))\n",
    "features_valid = count_tf_idf.transform(features_valid['lemm_text'].values.astype('U'))\n",
    "features_test = count_tf_idf.transform(features_test['lemm_text'].values.astype('U'))\n",
    "print(features_train.shape)\n",
    "print(features_valid.shape)\n",
    "print(features_test.shape)\n",
    "cv_counts = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificator = LogisticRegression()\n",
    "train_f1 = cross_val_score(classificator, \n",
    "                      features_train, \n",
    "                      target_train, \n",
    "                      cv=cv_counts, \n",
    "                      scoring='f1').mean()\n",
    "print('F1 на CV', train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Балансировка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Уменьшение размера весов классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_classes={0:1, 1:class_ratio}\n",
    "classificator = LogisticRegression(class_weight=dict_classes)\n",
    "train_f1_ballanced = cross_val_score(classificator, \n",
    "                                    features_train, \n",
    "                                    target_train, \n",
    "                                    cv=cv_counts, \n",
    "                                    scoring='f1').mean()\n",
    "print('F1 на CV с балансированными классами', train_f1_ballanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificator = LogisticRegression(class_weight='balanced')\n",
    "train_f1_balanced = cross_val_score(classificator, \n",
    "                                    features_train, \n",
    "                                    target_train, \n",
    "                                    cv=cv_counts, \n",
    "                                    scoring='f1').mean()\n",
    "print('F1 на CV с балансированными классами', train_f1_ballanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ресемплинг с уменшением класса 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_comments_train = toxic_comments.iloc[target_train.index]\n",
    "\n",
    "target_train_class_zero = toxic_comments_train[toxic_comments_train['toxic'] == 0]['toxic']\n",
    "target_train_class_one = toxic_comments_train[toxic_comments_train['toxic'] == 1]['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train_class_zero_downsample = target_train_class_zero.sample(target_train_class_one.shape[0],\n",
    "                                                                    random_state=12082020)\n",
    "target_train_downsample = pd.concat([target_train_class_zero_downsample, target_train_class_one])\n",
    "\n",
    "features_train_downsample = toxic_comments.iloc[target_train_downsample.index]\n",
    "features_train_downsample, target_train_downsample = shuffle(features_train_downsample,\n",
    "                                                             target_train_downsample,\n",
    "                                                             random_state=12082020)\n",
    "features_train_downsample = count_tf_idf.transform(features_train_downsample['lemm_text']\n",
    "                                                   .values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificator = LogisticRegression()\n",
    "train_f1_downsampled = cross_val_score(classificator,\n",
    "                      features_train_downsample, \n",
    "                      target_train_downsample, \n",
    "                      cv=cv_counts, \n",
    "                      scoring='f1').mean()\n",
    "print('F1 на CV с уменьшением классов', train_f1_downsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим значительный прирост f1 меры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построим ROC-AUC графики балансировки\n",
    "plt.figure(figsize=[12,9])\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='RandomModel')\n",
    "\n",
    "\n",
    "classificator = LogisticRegression()\n",
    "classificator.fit(features_train, target_train)\n",
    "probabilities_valid = classificator.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(target_valid, probabilities_one_valid)\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "valid_f1 = f1_score(target_valid, classificator.predict(features_valid))\n",
    "plt.plot(fpr, tpr, label='LR')\n",
    "\n",
    "classificator = LogisticRegression(class_weight=dict_classes)\n",
    "classificator.fit(features_train, target_train)\n",
    "probabilities_valid = classificator.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(target_valid, probabilities_one_valid)\n",
    "auc_roc_balanced = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "valid_f1_balanced = f1_score(target_valid, classificator.predict(features_valid))\n",
    "plt.plot(fpr, tpr, label='LR по весу')\n",
    "\n",
    "classificator = LogisticRegression()\n",
    "classificator.fit(features_train_downsample,target_train_downsample)\n",
    "probabilities_valid = classificator.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(target_valid, probabilities_one_valid)\n",
    "auc_roc_downsampled = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "valid_f1_downsampled = f1_score(target_valid, classificator.predict(features_valid))\n",
    "plt.plot(fpr, tpr, label='LR downsampled classes')\n",
    "\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "\n",
    "plt.legend(loc='lower right', fontsize='x-large')\n",
    "\n",
    "plt.title(\"ROC-кривая\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построим таблицу результатов баллансировки\n",
    "index = ['LogisticRegression',\n",
    "         'LR по весу',\n",
    "         'LR downsampled classes']\n",
    "data = {'F1 на CV':[train_f1,\n",
    "                    train_f1_balanced,\n",
    "                    train_f1_downsampled],\n",
    "        'F1 на валидации':[valid_f1,\n",
    "                           valid_f1_balanced,\n",
    "                           valid_f1_downsampled],\n",
    "        'AUC-ROC':[auc_roc,\n",
    "                   auc_roc_balanced,\n",
    "                   auc_roc_downsampled]}\n",
    "\n",
    "scores_data = pd.DataFrame(data=data, index=index)\n",
    "scores_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "1) В ходе подготовки данных мы получили признаки для обучения, получили обучающую, валидационную и тестовую выборку.\n",
    "\n",
    "2) По итогу балансировки даееых мо поняли что баланс по весам самый оптимальный, с ним и работаем дальше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификатор LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificator = LogisticRegression()\n",
    "hyperparams = [{'solver':['newton-cg', 'lbfgs', 'liblinear'],\n",
    "                'C':[0.1, 1, 10],\n",
    "                'class_weight':[dict_classes]}]\n",
    "\n",
    "\n",
    "print('# Tuning hyper-parameters for f1_score')\n",
    "print()\n",
    "clf = GridSearchCV(classificator, hyperparams, scoring='f1',cv=cv_counts)\n",
    "clf.fit(features_train, target_train)\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "LR_best_params = clf.best_params_\n",
    "print(LR_best_params)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.6f for %r\"% (mean, params))\n",
    "print()\n",
    "\n",
    "cv_f1_LR = max(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificator = LogisticRegression()\n",
    "classificator.set_params(**LR_best_params)\n",
    "classificator.fit(features_train, target_train)\n",
    "target_predict = classificator.predict(features_valid)\n",
    "valid_f1_LR = f1_score(target_valid, target_predict)\n",
    "print('F1 на cv', cv_f1_LR)\n",
    "print('F1 на валидации', valid_f1_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификатор DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificator = DecisionTreeClassifier()\n",
    "hyperparams = [{'max_depth':[x for x in range(50,100,2)],\n",
    "                'random_state':[1515],\n",
    "                'class_weight':[dict_classes]}]\n",
    "\n",
    "\n",
    "print('# Tuning hyper-parameters for f1_score')\n",
    "print()\n",
    "clf = GridSearchCV(classificator, hyperparams, scoring='f1',cv=cv_counts)\n",
    "clf.fit(features_train, target_train)\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "DTC_best_params = clf.best_params_\n",
    "print(DTC_best_params)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.6f for %r\"% (mean, params))\n",
    "print()\n",
    "\n",
    "cv_f1_DTC = max(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificator = DecisionTreeClassifier()\n",
    "classificator.set_params(**DTC_best_params)\n",
    "classificator.fit(features_train, target_train)\n",
    "target_predict = classificator.predict(features_valid)\n",
    "valid_f1_DTC = f1_score(target_valid, target_predict)\n",
    "print('F1 на cv', cv_f1_DTC)\n",
    "print('F1 на валидации', valid_f1_DTC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификатор CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificator = CatBoostClassifier(verbose=False, iterations=250)\n",
    "classificator.fit(features_train, target_train)\n",
    "target_predict = classificator.predict(features_valid)\n",
    "cv_f1_CBC = cross_val_score(classificator,\n",
    "                                         features_train, \n",
    "                                         target_train, \n",
    "                                         cv=cv_counts, \n",
    "                                         scoring='f1').mean()\n",
    "valid_f1_CBC = f1_score(target_valid, target_predict)\n",
    "print('F1 на cv', cv_f1_CBC)\n",
    "print('F1 на валидации', valid_f1_CBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод по классификаторам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['LogisticRegression',\n",
    "         'DecisionTreeClassifier',\n",
    "         'CatBoostClassifier']\n",
    "data = {'F1 на CV':[cv_f1_LR,\n",
    "                    cv_f1_DTC,\n",
    "                    cv_f1_CBC],\n",
    "        'F1 на валидации':[valid_f1_LR,\n",
    "                           valid_f1_DTC,\n",
    "                           valid_f1_CBC]}\n",
    "\n",
    "scores_data = pd.DataFrame(data=data, index=index)\n",
    "scores_data['Выполнение задачи'] = scores_data['F1 на валидации'] > 0.75\n",
    "scores_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "К тестовому набору данных перейдут LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,9])\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='RandomModel')\n",
    "\n",
    "\n",
    "classificator = LogisticRegression()\n",
    "classificator.set_params(**LR_best_params)\n",
    "classificator.fit(features_train, target_train)\n",
    "probabilities_test = classificator.predict_proba(features_test)\n",
    "probabilities_one_test = probabilities_test[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(target_test, probabilities_one_test)\n",
    "predict_test = classificator.predict(features_test)\n",
    "plt.plot(fpr, tpr, label='LogisticRegression')\n",
    "print('Метрики LogisticRegression')\n",
    "print('ROC AUC:', roc_auc_score(target_test, probabilities_one_test))\n",
    "print('F1:', f1_score(target_test, predict_test))\n",
    "print('Precision:', precision_score(target_test, predict_test))\n",
    "print('Recall:', recall_score(target_test, predict_test))\n",
    "print('Accuracy:', accuracy_score(target_test, predict_test))\n",
    "print()\n",
    "\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "\n",
    "plt.legend(loc='lower right', fontsize='x-large')\n",
    "\n",
    "plt.title(\"ROC-кривая\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вывод\n",
    "Было прделано:\n",
    "1) Подготовленны данные обучения.\n",
    "\n",
    "2) Выбран способ баланса классов, сформированы обучающая, валидационная и тестовая выборка.\n",
    "\n",
    "3) Обучены модели и выбрана лучшая из них на валидационной выборке.\n",
    "\n",
    "4) Показаны параметры качества моделей.\n",
    "\n",
    "**Исходные данные обладают большим количеством признаков.** Созданных столбцов больше, чем записей данных. Лучшая модель стала LogisticRegression. \n",
    "\n",
    "**CatBoostClassifier может показать себя очень хорошо при долгом обучении на данных.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [x]  Весь код выполняется без ошибок\n",
    "- [x]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [x]  Данные загружены и подготовлены\n",
    "- [x]  Модели обучены\n",
    "- [x]  Значение метрики *F1* не меньше 0.75\n",
    "- [x]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 3,
    "start_time": "2022-06-11T16:53:08.716Z"
   },
   {
    "duration": 780,
    "start_time": "2022-06-11T16:53:09.157Z"
   },
   {
    "duration": 356,
    "start_time": "2022-06-11T16:53:13.412Z"
   },
   {
    "duration": 840,
    "start_time": "2022-06-11T16:53:13.770Z"
   },
   {
    "duration": 12,
    "start_time": "2022-06-11T16:53:14.611Z"
   },
   {
    "duration": 11,
    "start_time": "2022-06-11T16:53:14.625Z"
   },
   {
    "duration": 63,
    "start_time": "2022-06-11T16:55:26.847Z"
   },
   {
    "duration": 1316,
    "start_time": "2022-06-11T16:55:43.721Z"
   },
   {
    "duration": 987,
    "start_time": "2022-06-11T16:55:46.851Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-11T16:56:00.475Z"
   },
   {
    "duration": 89744,
    "start_time": "2022-06-11T16:56:02.791Z"
   },
   {
    "duration": 20,
    "start_time": "2022-06-11T16:59:58.730Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-11T17:00:18.248Z"
   },
   {
    "duration": 270,
    "start_time": "2022-06-11T17:00:21.620Z"
   },
   {
    "duration": 3,
    "start_time": "2022-06-11T17:00:36.175Z"
   },
   {
    "duration": 62,
    "start_time": "2022-06-11T17:00:39.411Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-11T17:00:56.953Z"
   },
   {
    "duration": 10995,
    "start_time": "2022-06-11T17:00:59.629Z"
   },
   {
    "duration": 10,
    "start_time": "2022-06-11T17:30:02.361Z"
   },
   {
    "duration": 148,
    "start_time": "2022-06-11T17:30:21.280Z"
   },
   {
    "duration": 118634,
    "start_time": "2022-06-11T17:30:24.346Z"
   },
   {
    "duration": 12201,
    "start_time": "2022-06-11T17:34:04.473Z"
   },
   {
    "duration": 71572,
    "start_time": "2022-06-11T17:35:04.295Z"
   },
   {
    "duration": 43797,
    "start_time": "2022-06-11T17:36:15.869Z"
   },
   {
    "duration": 2,
    "start_time": "2022-06-11T17:40:54.142Z"
   },
   {
    "duration": 45,
    "start_time": "2022-06-11T17:41:07.952Z"
   },
   {
    "duration": 24,
    "start_time": "2022-06-11T17:41:15.284Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-11T17:41:32.368Z"
   },
   {
    "duration": 1038,
    "start_time": "2022-06-11T17:41:35.797Z"
   },
   {
    "duration": 37,
    "start_time": "2022-06-11T17:41:49.558Z"
   },
   {
    "duration": 27,
    "start_time": "2022-06-11T17:41:55.213Z"
   },
   {
    "duration": 303,
    "start_time": "2022-06-11T17:42:26.091Z"
   },
   {
    "duration": 20232,
    "start_time": "2022-06-11T17:42:43.536Z"
   },
   {
    "duration": 9,
    "start_time": "2022-06-11T17:44:46.880Z"
   },
   {
    "duration": 2122,
    "start_time": "2022-06-11T17:45:38.812Z"
   },
   {
    "duration": 2394,
    "start_time": "2022-06-11T17:45:40.936Z"
   },
   {
    "duration": 13,
    "start_time": "2022-06-11T17:45:43.331Z"
   },
   {
    "duration": 19,
    "start_time": "2022-06-11T17:45:43.346Z"
   },
   {
    "duration": 86524,
    "start_time": "2022-06-11T17:45:43.367Z"
   },
   {
    "duration": 10744,
    "start_time": "2022-06-11T17:47:09.892Z"
   },
   {
    "duration": 55823,
    "start_time": "2022-06-11T17:47:20.637Z"
   },
   {
    "duration": 73200,
    "start_time": "2022-06-11T17:48:16.462Z"
   },
   {
    "duration": 46899,
    "start_time": "2022-06-11T17:49:29.663Z"
   },
   {
    "duration": 36,
    "start_time": "2022-06-11T17:50:16.565Z"
   },
   {
    "duration": 1114,
    "start_time": "2022-06-11T17:50:16.602Z"
   },
   {
    "duration": 18541,
    "start_time": "2022-06-11T17:50:17.718Z"
   },
   {
    "duration": 122,
    "start_time": "2022-06-11T17:50:36.261Z"
   },
   {
    "duration": 14,
    "start_time": "2022-06-11T17:54:30.737Z"
   },
   {
    "duration": 758,
    "start_time": "2022-06-11T17:54:45.527Z"
   },
   {
    "duration": 88989,
    "start_time": "2022-06-11T17:54:53.765Z"
   },
   {
    "duration": 11,
    "start_time": "2022-06-11T17:56:32.775Z"
   },
   {
    "duration": 11,
    "start_time": "2022-06-11T17:57:28.822Z"
   },
   {
    "duration": 20,
    "start_time": "2022-06-11T18:03:08.218Z"
   },
   {
    "duration": 336064,
    "start_time": "2022-06-11T18:03:42.144Z"
   },
   {
    "duration": 30370,
    "start_time": "2022-06-11T18:10:20.950Z"
   },
   {
    "duration": 39,
    "start_time": "2022-06-11T18:11:33.199Z"
   },
   {
    "duration": 16,
    "start_time": "2022-06-11T18:12:17.232Z"
   },
   {
    "duration": 281591,
    "start_time": "2022-06-11T18:14:31.133Z"
   },
   {
    "duration": 712432,
    "start_time": "2022-06-11T18:19:59.976Z"
   },
   {
    "duration": 37754,
    "start_time": "2022-06-11T18:38:40.106Z"
   },
   {
    "duration": 726547,
    "start_time": "2022-06-11T18:42:54.867Z"
   },
   {
    "duration": 15,
    "start_time": "2022-06-11T18:55:01.416Z"
   },
   {
    "duration": 31719,
    "start_time": "2022-06-11T18:59:24.984Z"
   },
   {
    "duration": 1362,
    "start_time": "2022-06-11T19:08:53.314Z"
   },
   {
    "duration": 917,
    "start_time": "2022-06-11T19:08:54.677Z"
   },
   {
    "duration": 9,
    "start_time": "2022-06-11T19:08:55.596Z"
   },
   {
    "duration": 10,
    "start_time": "2022-06-11T19:08:55.607Z"
   },
   {
    "duration": 97871,
    "start_time": "2022-06-11T19:08:55.619Z"
   },
   {
    "duration": 12105,
    "start_time": "2022-06-11T19:10:33.492Z"
   },
   {
    "duration": 70874,
    "start_time": "2022-06-11T19:10:45.598Z"
   },
   {
    "duration": 91512,
    "start_time": "2022-06-11T19:11:56.474Z"
   },
   {
    "duration": 5557,
    "start_time": "2022-06-12T05:57:02.492Z"
   },
   {
    "duration": 2535,
    "start_time": "2022-06-12T05:57:08.053Z"
   },
   {
    "duration": 11,
    "start_time": "2022-06-12T05:57:10.590Z"
   },
   {
    "duration": 15,
    "start_time": "2022-06-12T05:57:10.602Z"
   },
   {
    "duration": 93933,
    "start_time": "2022-06-12T05:57:10.618Z"
   },
   {
    "duration": 11151,
    "start_time": "2022-06-12T05:58:44.553Z"
   },
   {
    "duration": 55662,
    "start_time": "2022-06-12T05:58:55.706Z"
   },
   {
    "duration": 72695,
    "start_time": "2022-06-12T05:59:51.370Z"
   },
   {
    "duration": 46504,
    "start_time": "2022-06-12T06:01:04.067Z"
   },
   {
    "duration": 1888,
    "start_time": "2022-06-12T06:01:50.574Z"
   },
   {
    "duration": 997,
    "start_time": "2022-06-12T06:01:52.464Z"
   },
   {
    "duration": 17702,
    "start_time": "2022-06-12T06:01:53.463Z"
   },
   {
    "duration": 71600,
    "start_time": "2022-06-12T06:03:37.078Z"
   },
   {
    "duration": 45803,
    "start_time": "2022-06-12T06:04:48.679Z"
   },
   {
    "duration": 25,
    "start_time": "2022-06-12T06:05:34.484Z"
   },
   {
    "duration": 1063,
    "start_time": "2022-06-12T06:05:34.510Z"
   },
   {
    "duration": 18893,
    "start_time": "2022-06-12T06:05:35.574Z"
   },
   {
    "duration": 88527,
    "start_time": "2022-06-12T06:05:54.471Z"
   },
   {
    "duration": 10,
    "start_time": "2022-06-12T06:07:23.000Z"
   },
   {
    "duration": 325845,
    "start_time": "2022-06-12T06:08:34.258Z"
   },
   {
    "duration": 32706,
    "start_time": "2022-06-12T06:14:00.105Z"
   },
   {
    "duration": 1442,
    "start_time": "2022-06-13T09:04:22.603Z"
   },
   {
    "duration": 107,
    "start_time": "2022-06-13T09:04:45.629Z"
   },
   {
    "duration": 1276,
    "start_time": "2022-06-13T09:04:52.684Z"
   },
   {
    "duration": 3004,
    "start_time": "2022-06-13T09:04:53.962Z"
   },
   {
    "duration": 9,
    "start_time": "2022-06-13T09:04:56.967Z"
   },
   {
    "duration": 8,
    "start_time": "2022-06-13T09:04:56.978Z"
   },
   {
    "duration": 140,
    "start_time": "2022-06-13T09:04:56.988Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:04:57.130Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:04:57.132Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:04:57.133Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:04:57.134Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:04:57.135Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:04:57.136Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:04:57.137Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:04:57.139Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:04:57.140Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:04:57.141Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:04:57.142Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:04:57.144Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:04:57.145Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:04:57.147Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:04:57.148Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:04:57.149Z"
   },
   {
    "duration": 5770,
    "start_time": "2022-06-13T09:05:27.981Z"
   },
   {
    "duration": 9439,
    "start_time": "2022-06-13T09:05:39.285Z"
   },
   {
    "duration": 1420,
    "start_time": "2022-06-13T09:06:11.866Z"
   },
   {
    "duration": 698,
    "start_time": "2022-06-13T09:06:13.287Z"
   },
   {
    "duration": 12,
    "start_time": "2022-06-13T09:06:13.986Z"
   },
   {
    "duration": 9,
    "start_time": "2022-06-13T09:06:13.999Z"
   },
   {
    "duration": 5845,
    "start_time": "2022-06-13T09:06:14.010Z"
   },
   {
    "duration": 9816,
    "start_time": "2022-06-13T09:06:19.856Z"
   },
   {
    "duration": 48827,
    "start_time": "2022-06-13T09:06:29.674Z"
   },
   {
    "duration": 54002,
    "start_time": "2022-06-13T09:07:18.503Z"
   },
   {
    "duration": 41586,
    "start_time": "2022-06-13T09:08:12.506Z"
   },
   {
    "duration": 24,
    "start_time": "2022-06-13T09:08:54.094Z"
   },
   {
    "duration": 939,
    "start_time": "2022-06-13T09:08:54.119Z"
   },
   {
    "duration": 19140,
    "start_time": "2022-06-13T09:08:55.060Z"
   },
   {
    "duration": 8,
    "start_time": "2022-06-13T09:09:21.393Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:09:21.402Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:09:21.403Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:09:21.404Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:09:21.405Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:09:21.405Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:09:21.406Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:09:21.407Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:09:21.408Z"
   },
   {
    "duration": 39,
    "start_time": "2022-06-13T09:09:21.410Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:09:21.450Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:09:21.451Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:09:21.452Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:09:21.453Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:09:21.454Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:09:21.454Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:09:21.491Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:09:21.492Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:09:21.493Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:09:21.494Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:09:21.495Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-13T09:09:21.495Z"
   },
   {
    "duration": 3,
    "start_time": "2022-06-13T09:10:03.101Z"
   },
   {
    "duration": 1295,
    "start_time": "2022-06-13T09:10:34.881Z"
   },
   {
    "duration": 678,
    "start_time": "2022-06-13T09:10:36.179Z"
   },
   {
    "duration": 8,
    "start_time": "2022-06-13T09:10:36.859Z"
   },
   {
    "duration": 23,
    "start_time": "2022-06-13T09:10:36.869Z"
   },
   {
    "duration": 5793,
    "start_time": "2022-06-13T09:10:36.894Z"
   },
   {
    "duration": 9236,
    "start_time": "2022-06-13T09:10:42.688Z"
   },
   {
    "duration": 45171,
    "start_time": "2022-06-13T09:10:51.925Z"
   },
   {
    "duration": 1558,
    "start_time": "2022-06-13T09:12:14.570Z"
   },
   {
    "duration": 900,
    "start_time": "2022-06-13T09:12:16.129Z"
   },
   {
    "duration": 12,
    "start_time": "2022-06-13T09:12:17.031Z"
   },
   {
    "duration": 15,
    "start_time": "2022-06-13T09:12:17.046Z"
   },
   {
    "duration": 5881,
    "start_time": "2022-06-13T09:12:17.063Z"
   },
   {
    "duration": 9198,
    "start_time": "2022-06-13T09:12:22.946Z"
   },
   {
    "duration": 45056,
    "start_time": "2022-06-13T09:12:32.145Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
